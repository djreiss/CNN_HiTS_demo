{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network applied to Transient Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put data loader into a function, break up potentially into train/valid/test subsets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pkl_data(chunk_num, split_frac=(0.8, 0.9), verbose=False):\n",
    "    fname = 'all_chunks/chunk_%d_5000.pkl.gz' % chunk_num\n",
    "    pkl_data = np.load(gzip.GzipFile(fname, 'rb'), encoding='bytes')\n",
    "    if False:\n",
    "        print(pkl_data.keys())\n",
    "        print(pkl_data[b'diff_images'].shape)\n",
    "    \n",
    "    N_data = pkl_data[b'diff_images'].shape[0]\n",
    "    if False:\n",
    "        print(N_data)\n",
    "    X = np.array([pkl_data[b'temp_images'].reshape((N_data, 21, 21)), \n",
    "                 pkl_data[b'sci_images'].reshape((N_data, 21, 21)),\n",
    "                 pkl_data[b'diff_images'].reshape((N_data, 21, 21)),\n",
    "                 pkl_data[b'SNR_images'].reshape((N_data, 21, 21))])\n",
    "    X = np.swapaxes(X, 0, 1)\n",
    "\n",
    "    Y = np.array([np.logical_not(pkl_data[b'labels']), pkl_data[b'labels']]).transpose()\n",
    "    if False:\n",
    "        print(X.shape, Y.shape)\n",
    "        \n",
    "    N_train = int(N_data * split_frac[0])\n",
    "    N_valid = 0\n",
    "    if split_frac[0] < 1.0:\n",
    "        N_valid = int(N_data * split_frac[1])\n",
    "    N_test = 0\n",
    "    if split_frac[1] < 1.0:\n",
    "        N_test  = int(N_data * 1.0)\n",
    "\n",
    "    X_train, Y_train = X[:N_train], Y[:N_train]\n",
    "    X_valid = Y_valid = None\n",
    "    if N_valid > 0:\n",
    "        X_valid, Y_valid = X[N_train:N_valid], Y[N_train:N_valid]\n",
    "    X_test = Y_test = None\n",
    "    if N_test > 0:\n",
    "        X_test, Y_test = X[N_valid:N_test], Y[N_valid:N_test]\n",
    "\n",
    "    if verbose:\n",
    "        print(np.mean(Y[:,0]), np.mean(Y[:,1]))\n",
    "        print(\"Train: \", X_train.shape, Y_train.shape)\n",
    "        if N_valid > 0:\n",
    "            print(\"Valid: \", X_valid.shape, Y_valid.shape)\n",
    "        if N_test > 0:\n",
    "            print(\"Test: \", X_test.shape, Y_test.shape)\n",
    "        \n",
    "    return (X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n",
      "Train:  (4527, 4, 21, 21) (4527, 2)\n",
      "Valid:  (503, 4, 21, 21) (503, 2)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), _ = load_pkl_data(188, (0.9, 1.0), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a Keras sequential model and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "\n",
    "# set dimensions ordering (depth as index 1)\n",
    "import keras\n",
    "keras.backend.set_image_dim_ordering('th')\n",
    "\n",
    "def make_model(compile=True, epochs=100, lrate=0.01, decay=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((3, 3), input_shape = (4, 21, 21)))\n",
    "    model.add(Convolution2D(32, (4, 4), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    if epochs <= 2:\n",
    "        model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    if epochs <= 2:\n",
    "        model.add(Dropout(0.1))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    if compile:\n",
    "        # model.compile(loss='mean_squared_error',\n",
    "        #       optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "        # initiate RMSprop optimizer (OLD)\n",
    "        #opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "        # Compile model\n",
    "        #epochs = 25\n",
    "        #lrate = 0.01\n",
    "        if decay is None:\n",
    "            if epochs > 2:\n",
    "                decay = lrate/epochs\n",
    "            else:\n",
    "                decay = lrate/100.\n",
    "        opt = keras.optimizers.SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "\n",
    "        # Let's train the model using RMSprop\n",
    "        model.compile(loss='mean_squared_error', #categorical_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "model = make_model(epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 4, 27, 27)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 24, 24)        2080      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 32, 26, 26)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 24, 24)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 32, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 12, 12)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 64, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 12, 12)        36928     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 64, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 12, 12)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 255,490.0\n",
      "Trainable params: 255,490.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit our model to the training data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, train, valid, epochs=25, batch_size=32, data_augmentation=True, \n",
    "              patience=5, **kwargs):\n",
    "    X_train, Y_train = train\n",
    "    X_valid, Y_valid = valid\n",
    "    \n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "    if not data_augmentation:\n",
    "        if epochs > 2:\n",
    "            print('Not using data augmentation.')\n",
    "        histry = model.fit(X_train, Y_train, batch_size=batch_size, \n",
    "                  epochs=epochs, validation_data=(X_valid, Y_valid),\n",
    "                  shuffle=True, callbacks=[early_stopping], **kwargs)\n",
    "    else:\n",
    "        if epochs > 2:\n",
    "            print('Using real-time data augmentation.')\n",
    "        from keras.preprocessing.image import ImageDataGenerator\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True)  # randomly flip images\n",
    "\n",
    "        # Compute quantities required for feature-wise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        # datagen.fit(X_train)  # so not needed.\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        histry = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), \n",
    "                            epochs=epochs, validation_data=(X_valid, Y_valid),\n",
    "                            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                            callbacks=[early_stopping], **kwargs) \n",
    "        \n",
    "    return model, histry"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "model = make_model()\n",
    "model, histry = run_model(model, (X_train, Y_train), (X_valid, Y_valid), data_augmentation=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.seed(seed)\n",
    "model = make_model()\n",
    "model, histry = run_model(model, (X_train, Y_train), (X_valid, Y_valid), data_augmentation=True,\n",
    "                         patience=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import metrics\n",
    "pred = model.predict_classes(X_test)\n",
    "print(metrics.classification_report(Y_test[:,1].astype(int), pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test[:,1].astype(int), pred)\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR (recall)')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ypred = pred\n",
    "ytest = Y_test[:, 1].astype(int)\n",
    "\n",
    "N_plot = 10\n",
    "only_plot_wrong = True\n",
    "if not only_plot_wrong:\n",
    "    plot_inds = range(N_plot)\n",
    "else:\n",
    "    plot_inds = np.where(ypred != ytest)[0]\n",
    "    if len(plot_inds) > N_plot:\n",
    "        plot_inds = plot_inds[:N_plot]\n",
    "N_plot = len(plot_inds)\n",
    "\n",
    "plt.clf()\n",
    "fig, axes = plt.subplots(N_plot, 4, figsize=(4, N_plot*1.2),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "i = 0\n",
    "for ind in plot_inds:\n",
    "    axes.flat[4*i].imshow(X_test[ind][0], interpolation = \"none\")\n",
    "    axes.flat[4*i + 1].imshow(X_test[ind][1], interpolation = \"none\")\n",
    "    axes.flat[4*i + 2].imshow(X_test[ind][2], interpolation = \"none\")\n",
    "    axes.flat[4*i + 3].imshow(X_test[ind][3], interpolation = \"none\")\n",
    "\n",
    "    axes.flat[4*i + 3].set_title (\"predicted pbb = \" + str(np.round(ypred[ind], 2)) + \n",
    "                                  \", label = \" + str(ytest[ind]))\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try fitting a large number of data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data = 100  # number of datasets to run\n",
    "seed = 666\n",
    "batch_size = 32\n",
    "epochs = 1  # 10   # Probably want to stop around 25 but now we have auto-stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0: Test loss = 0.020522; Test accuracy = 0.977011\n",
      "Dataset 1: Test loss = 0.021069; Test accuracy = 0.978261\n",
      "Dataset 2: Test loss = 0.047351; Test accuracy = 0.952096\n",
      "Dataset 3: Test loss = 0.016788; Test accuracy = 0.986193\n",
      "Dataset 4: Test loss = 0.014735; Test accuracy = 0.984095\n",
      "Dataset 5: Test loss = 0.025702; Test accuracy = 0.968992\n",
      "Dataset 6: Test loss = 0.014422; Test accuracy = 0.982143\n",
      "Dataset 7: Test loss = 0.035030; Test accuracy = 0.956436\n",
      "Dataset 8: Test loss = 0.033313; Test accuracy = 0.962151\n",
      "Dataset 9: Test loss = 0.017067; Test accuracy = 0.984032\n",
      "Dataset 10: Test loss = 0.006057; Test accuracy = 0.992395\n",
      "Dataset 11: Test loss = 0.015210; Test accuracy = 0.984586\n",
      "Dataset 12: Test loss = 0.027590; Test accuracy = 0.968064\n",
      "Dataset 13: Test loss = 0.026422; Test accuracy = 0.970120\n",
      "Dataset 14: Test loss = 0.010145; Test accuracy = 0.990291\n",
      "Dataset 15: Test loss = 0.015365; Test accuracy = 0.984000\n",
      "Dataset 16: Test loss = 0.009928; Test accuracy = 0.988350\n",
      "Dataset 17: Test loss = 0.007312; Test accuracy = 0.992322\n",
      "Dataset 18: Test loss = 0.018026; Test accuracy = 0.980315\n",
      "Dataset 19: Test loss = 0.021313; Test accuracy = 0.974308\n",
      "Dataset 20: Test loss = 0.011494; Test accuracy = 0.986056\n",
      "Dataset 21: Test loss = 0.017901; Test accuracy = 0.980276\n",
      "Dataset 22: Test loss = 0.042005; Test accuracy = 0.954274\n",
      "Dataset 23: Test loss = 0.016604; Test accuracy = 0.982490\n",
      "Dataset 24: Test loss = 0.008004; Test accuracy = 0.992095\n",
      "Dataset 25: Test loss = 0.027422; Test accuracy = 0.964427\n",
      "Dataset 26: Test loss = 0.012517; Test accuracy = 0.986111\n",
      "Dataset 27: Test loss = 0.018306; Test accuracy = 0.980080\n",
      "Dataset 28: Test loss = 0.024154; Test accuracy = 0.972441\n",
      "Dataset 29: Test loss = 0.011394; Test accuracy = 0.986139\n",
      "Dataset 30: Test loss = 0.007466; Test accuracy = 0.992188\n",
      "Dataset 31: Test loss = 0.003893; Test accuracy = 0.996071\n",
      "Dataset 32: Test loss = 0.003397; Test accuracy = 0.996008\n",
      "Dataset 33: Test loss = 0.030312; Test accuracy = 0.962000\n",
      "Dataset 34: Test loss = 0.014112; Test accuracy = 0.986193\n",
      "Dataset 35: Test loss = 0.007312; Test accuracy = 0.992079\n",
      "Dataset 36: Test loss = 0.009854; Test accuracy = 0.990291\n",
      "Dataset 37: Test loss = 0.008907; Test accuracy = 0.990548\n",
      "Dataset 38: Test loss = 0.038709; Test accuracy = 0.958743\n",
      "Dataset 39: Test loss = 0.005501; Test accuracy = 0.994175\n",
      "Dataset 40: Test loss = 0.023855; Test accuracy = 0.974155\n",
      "Dataset 41: Test loss = 0.010680; Test accuracy = 0.990157\n",
      "Dataset 42: Test loss = 0.007423; Test accuracy = 0.992157\n",
      "Dataset 43: Test loss = 0.018265; Test accuracy = 0.980315\n",
      "Dataset 44: Test loss = 0.029711; Test accuracy = 0.968254\n",
      "Dataset 45: Test loss = 0.027818; Test accuracy = 0.970060\n",
      "Dataset 46: Test loss = 0.010913; Test accuracy = 0.990020\n",
      "Dataset 47: Test loss = 0.007599; Test accuracy = 0.992079\n",
      "Dataset 48: Test loss = 0.012030; Test accuracy = 0.986083\n",
      "Dataset 49: Test loss = 0.012365; Test accuracy = 0.984221\n",
      "Dataset 50: Test loss = 0.008243; Test accuracy = 0.992172\n",
      "Dataset 51: Test loss = 0.005491; Test accuracy = 0.992110\n",
      "Dataset 52: Test loss = 0.020311; Test accuracy = 0.974560\n",
      "Dataset 53: Test loss = 0.013201; Test accuracy = 0.986083\n",
      "Dataset 54: Test loss = 0.009248; Test accuracy = 0.990099\n",
      "Dataset 55: Test loss = 0.017546; Test accuracy = 0.980080\n",
      "Dataset 56: Test loss = 0.014373; Test accuracy = 0.984064\n",
      "Dataset 57: Test loss = 0.007096; Test accuracy = 0.992218\n",
      "Dataset 58: Test loss = 0.002786; Test accuracy = 0.998039\n",
      "Dataset 59: Test loss = 0.045632; Test accuracy = 0.950100\n",
      "Dataset 60: Test loss = 0.007654; Test accuracy = 0.992233\n",
      "Dataset 61: Test loss = 0.027581; Test accuracy = 0.970000\n",
      "Dataset 62: Test loss = 0.017972; Test accuracy = 0.982213\n",
      "Dataset 63: Test loss = 0.014309; Test accuracy = 0.984190\n",
      "Dataset 64: Test loss = 0.003468; Test accuracy = 0.996078\n",
      "Dataset 65: Test loss = 0.004210; Test accuracy = 0.994012\n",
      "Dataset 66: Test loss = 0.008620; Test accuracy = 0.992172\n",
      "Dataset 67: Test loss = 0.009051; Test accuracy = 0.984190\n",
      "Dataset 68: Test loss = 0.023227; Test accuracy = 0.976048\n",
      "Dataset 69: Test loss = 0.012315; Test accuracy = 0.988000\n",
      "Dataset 70: Test loss = 0.020989; Test accuracy = 0.978431\n",
      "Dataset 71: Test loss = 0.010652; Test accuracy = 0.990079\n",
      "Dataset 72: Test loss = 0.013154; Test accuracy = 0.986166\n",
      "Dataset 73: Test loss = 0.014732; Test accuracy = 0.984000\n",
      "Dataset 74: Test loss = 0.008833; Test accuracy = 0.992048\n",
      "Dataset 75: Test loss = 0.021622; Test accuracy = 0.970414\n",
      "Dataset 76: Test loss = 0.011262; Test accuracy = 0.988166\n",
      "Dataset 77: Test loss = 0.014277; Test accuracy = 0.986355\n",
      "Dataset 78: Test loss = 0.013309; Test accuracy = 0.984158\n",
      "Dataset 79: Test loss = 0.013493; Test accuracy = 0.984496\n",
      "Dataset 80: Test loss = 0.018286; Test accuracy = 0.976331\n",
      "Dataset 81: Test loss = 0.014180; Test accuracy = 0.984032\n",
      "Dataset 82: Test loss = 0.010508; Test accuracy = 0.990020\n",
      "Dataset 83: Test loss = 0.011286; Test accuracy = 0.988142\n",
      "Dataset 84: Test loss = 0.008127; Test accuracy = 0.992000\n",
      "Dataset 85: Test loss = 0.004851; Test accuracy = 0.994106\n",
      "Dataset 86: Test loss = 0.019283; Test accuracy = 0.976000\n",
      "Dataset 87: Test loss = 0.011345; Test accuracy = 0.982759\n",
      "Dataset 88: Test loss = 0.022436; Test accuracy = 0.976096\n",
      "Dataset 89: Test loss = 0.019445; Test accuracy = 0.980237\n",
      "Dataset 90: Test loss = 0.016706; Test accuracy = 0.980040\n",
      "Dataset 91: Test loss = 0.012087; Test accuracy = 0.984190\n",
      "Dataset 92: Test loss = 0.017169; Test accuracy = 0.978088\n",
      "Dataset 93: Test loss = 0.040439; Test accuracy = 0.956175\n",
      "Dataset 94: Test loss = 0.013836; Test accuracy = 0.986248\n",
      "Dataset 95: Test loss = 0.016497; Test accuracy = 0.980315\n",
      "Dataset 96: Test loss = 0.006207; Test accuracy = 0.994275\n",
      "Dataset 97: Test loss = 0.009040; Test accuracy = 0.992032\n",
      "Dataset 98: Test loss = 0.019722; Test accuracy = 0.980040\n",
      "Dataset 99: Test loss = 0.009809; Test accuracy = 0.990253\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "model = make_model()\n",
    "\n",
    "for iter in range(100:)\n",
    "    for d in range(n_data):\n",
    "        (X_train, Y_train), (X_valid, Y_valid), (X_test, Y_test) = load_pkl_data(d)\n",
    "        _, history = run_model(model, (X_train, Y_train), (X_valid, Y_valid), \n",
    "                  data_augmentation=True, verbose=0)\n",
    "        score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print('Iter %d, Dataset %d: Test loss = %f; Test accuracy = %f' % (iter, d, score[0], score[1]))\n",
    "        if iter > 0 and iter % 10 == 0:\n",
    "            model.save('models/CNN_HiTS_demo_big_%02d.hdf5' % iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/CNN_HiTS_demo_big_FINAL.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test[:,1].astype(int), pred)\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR (recall)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using `fit_generator()` with a data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "imageDatagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5062, 4, 21, 21) (5062, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 4, 21, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = 3\n",
    "(X, Y), _, _ = load_pkl_data(chunk, split_frac=(1.0, 1.0), verbose=False)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "tmp = imageDatagen.flow(X, Y, batch_size=32).next()\n",
    "tmp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def data_generator_train():\n",
    "    data_range = np.arange(200)  # use inds 0-150 for training\n",
    "    while True:\n",
    "        chunk = np.random.choice(data_range, size=1)[0]\n",
    "        print('TRAIN:', chunk)\n",
    "        (X, Y), _, _ = load_pkl_data(chunk, split_frac=(1.0, 1.0), verbose=False)\n",
    "        for i in range(X.shape[0]//batch_size//2):  # about 5000/32/2 = 150//2\n",
    "            Xb, Yb = imageDatagen.flow(X, Y, batch_size=batch_size).next()\n",
    "            yield (Xb, Yb)\n",
    "        #yield (X, Y)\n",
    "\n",
    "def data_generator_valid():\n",
    "    data_range = np.arange(151, 235)  # use inds 151-234 for validation\n",
    "    while True:\n",
    "        chunk = np.random.choice(data_range, size=1)[0]\n",
    "        print('VALID:', chunk)\n",
    "        (X, Y), _, _ = load_pkl_data(chunk, split_frac=(1.0, 1.0), verbose=False)\n",
    "        for i in range(X.shape[0]//batch_size//2): # about 5000/32/2 = 150//2\n",
    "            Xb, Yb = imageDatagen.flow(X, Y, batch_size=batch_size).next()\n",
    "            yield (Xb, Yb)\n",
    "        #yield (X, Y)\n",
    "        \n",
    "# Then we can use inds 235-284 as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "\n",
    "train_generator = data_generator_train()\n",
    "valid_generator = data_generator_valid()\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1000)\n",
    "checkpointing = ModelCheckpoint('./best_model.hdf5', monitor='val_loss', verbose=1, \n",
    "                                save_best_only=True, save_weights_only=False, \n",
    "                                mode='auto', period=1)\n",
    "\n",
    "model.fit_generator(generator=train_generator, \n",
    "                    validation_data = valid_generator, validation_steps=1,\n",
    "                    #datagen.flow(X_train, Y_train, batch_size=batch_size), \n",
    "                    epochs=epochs, steps_per_epoch=100,\n",
    "                    callbacks=[early_stopping, checkpointing], workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
